# ðŸ§ª LLM Benchmarking Tool

This project benchmarks the performance of open-source LLMs (like LLaMA 3.1 8B, Qwen 2.5, and Gemma 2B) on local hardware.

## ðŸ“Œ Features
- Benchmark LLMs for latency, memory usage, and tokens-per-minute (TPM)
- Lightweight benchmarking script for local environments
- YAML-based config system for easy model switching
- Results logging (coming soon)

## ðŸ“‚ Folder Structure
```
LLM_Benchmarking/
â”œâ”€â”€ configs/         # YAML config files
â”œâ”€â”€ models/          # Pre-downloaded models (not versioned)
â”œâ”€â”€ results/         # Benchmark logs/output
â”œâ”€â”€ scripts/         # Python benchmark logic
â”œâ”€â”€ requirements.txt # Dependency list
â””â”€â”€ README.md
```

## ðŸš€ Setup Instructions

### 1. Create Virtual Environment (optional but recommended)
```bash
python -m venv venv
venv\Scripts\activate    # Windows
source venv/bin/activate   # Mac/Linux
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Run Benchmark
```bash
cd scripts
python run_all.py
```

## ðŸ§¹ Important: Exclude Large Files

This repo **does not** track:
- `venv/`
- `.pyc`, `.pkl`, `.log`
- Model binaries

> Make sure your `.gitignore` file includes these to avoid GitHub push issues.

## ðŸ§  Models in Use
```yaml
- meta-llama/Llama-3-8B-Instruct
- Qwen/Qwen-2B-Instruct
- google/gemma-2b-it
```

## ðŸ“œ License
MIT License Â© 2025
